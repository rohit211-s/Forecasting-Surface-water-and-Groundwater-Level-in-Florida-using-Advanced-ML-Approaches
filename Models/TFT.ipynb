{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e667a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohithreddynedhunuri/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd17b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95120a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1e85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import optuna.logging\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "optuna_logger = logging.getLogger(\"optuna\")\n",
    "\n",
    "class PyTorchLightningPruningCallbackAdjusted(PyTorchLightningPruningCallback):\n",
    "  pass\n",
    "\n",
    "def optimize_hyperparameters(\n",
    "    train_dataloaders: DataLoader,\n",
    "    val_dataloaders: DataLoader,\n",
    "    model_path: str,\n",
    "    max_epochs: int = 20,\n",
    "    n_trials: int = 100,\n",
    "    timeout: float = 3600 * 8.0,  # 8 hours\n",
    "    gradient_clip_val_range: Tuple[float, float] = (0.01, 100.0),\n",
    "    hidden_size_range: Tuple[int, int] = (16, 265),\n",
    "    hidden_continuous_size_range: Tuple[int, int] = (8, 64),\n",
    "    attention_head_size_range: Tuple[int, int] = (1, 4),\n",
    "    dropout_range: Tuple[float, float] = (0.1, 0.3),\n",
    "    learning_rate_range: Tuple[float, float] = (1e-5, 1.0),\n",
    "    use_learning_rate_finder: bool = True,\n",
    "    trainer_kwargs: Dict[str, Any] = {},\n",
    "    log_dir: str = \"lightning_logs\",\n",
    "    study: optuna.Study = None,\n",
    "    verbose: Union[int, bool] = None,\n",
    "    pruner: optuna.pruners.BasePruner = optuna.pruners.SuccessiveHalvingPruner(),\n",
    "    **kwargs,\n",
    ") -> optuna.Study:\n",
    "      assert isinstance(train_dataloaders.dataset, TimeSeriesDataSet) and isinstance(\n",
    "        val_dataloaders.dataset, TimeSeriesDataSet\n",
    "    ), \"dataloaders must be built from timeseriesdataset\"\n",
    "\n",
    "      logging_level = {\n",
    "        None: optuna.logging.get_verbosity(),\n",
    "        0: optuna.logging.WARNING,\n",
    "        1: optuna.logging.INFO,\n",
    "        2: optuna.logging.DEBUG,\n",
    "    }\n",
    "      optuna_verbose = logging_level[verbose]\n",
    "      optuna.logging.set_verbosity(optuna_verbose)\n",
    "\n",
    "      loss = kwargs.get(\n",
    "        \"loss\", QuantileLoss()\n",
    "    )  # need a deepcopy of loss as it will otherwise propagate from one trial to the next\n",
    "\n",
    "    # create objective function\n",
    "      def objective(trial: optuna.Trial) -> float:\n",
    "        # Filenames for each trial must be made unique in order to access each checkpoint.\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=os.path.join(model_path, \"trial_{}\".format(trial.number)), filename=\"{epoch}\", monitor=\"val_loss\"\n",
    "        )\n",
    "\n",
    "        learning_rate_callback = LearningRateMonitor()\n",
    "        logger = TensorBoardLogger(log_dir, name=\"optuna\", version=trial.number)\n",
    "        gradient_clip_val = trial.suggest_loguniform(\"gradient_clip_val\", *gradient_clip_val_range)\n",
    "        default_trainer_kwargs = dict(\n",
    "            accelerator=\"auto\",\n",
    "            max_epochs=max_epochs,\n",
    "            gradient_clip_val=gradient_clip_val,\n",
    "            callbacks=[\n",
    "                learning_rate_callback,\n",
    "                checkpoint_callback,\n",
    "                PyTorchLightningPruningCallbackAdjusted(trial, monitor=\"val_loss\"),\n",
    "            ],\n",
    "            logger=logger,\n",
    "            enable_progress_bar=optuna_verbose < optuna.logging.INFO,\n",
    "            enable_model_summary=[False, True][optuna_verbose < optuna.logging.INFO],\n",
    "        )\n",
    "        default_trainer_kwargs.update(trainer_kwargs)\n",
    "        trainer = pl.Trainer(\n",
    "            **default_trainer_kwargs,\n",
    "        )\n",
    "\n",
    "        # create model\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", *hidden_size_range, log=True)\n",
    "        kwargs[\"loss\"] = copy.deepcopy(loss)\n",
    "        model = TemporalFusionTransformer.from_dataset(\n",
    "            train_dataloaders.dataset,\n",
    "            dropout=trial.suggest_uniform(\"dropout\", *dropout_range),\n",
    "            hidden_size=hidden_size,\n",
    "            hidden_continuous_size=trial.suggest_int(\n",
    "                \"hidden_continuous_size\",\n",
    "                hidden_continuous_size_range[0],\n",
    "                min(hidden_continuous_size_range[1], hidden_size),\n",
    "                log=True,\n",
    "            ),\n",
    "            attention_head_size=trial.suggest_int(\"attention_head_size\", *attention_head_size_range),\n",
    "            log_interval=-1,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # find good learning rate\n",
    "        if use_learning_rate_finder:\n",
    "            lr_trainer = pl.Trainer(\n",
    "                gradient_clip_val=gradient_clip_val,\n",
    "                accelerator=\"auto\",\n",
    "                logger=False,\n",
    "                enable_progress_bar=False,\n",
    "                enable_model_summary=False,\n",
    "            )\n",
    "            tuner = Tuner(lr_trainer)\n",
    "            res = tuner.lr_find(\n",
    "                model,\n",
    "                train_dataloaders=train_dataloaders,\n",
    "                val_dataloaders=val_dataloaders,\n",
    "                early_stop_threshold=10000,\n",
    "                min_lr=learning_rate_range[0],\n",
    "                num_training=100,\n",
    "                max_lr=learning_rate_range[1],\n",
    "            )\n",
    "\n",
    "            loss_finite = np.isfinite(res.results[\"loss\"])\n",
    "            if loss_finite.sum() > 3:  # at least 3 valid values required for learning rate finder\n",
    "                lr_smoothed, loss_smoothed = sm.nonparametric.lowess(\n",
    "                    np.asarray(res.results[\"loss\"])[loss_finite],\n",
    "                    np.asarray(res.results[\"lr\"])[loss_finite],\n",
    "                    frac=1.0 / 10.0,\n",
    "                )[min(loss_finite.sum() - 3, 10) : -1].T\n",
    "                optimal_idx = np.gradient(loss_smoothed).argmin()\n",
    "                optimal_lr = lr_smoothed[optimal_idx]\n",
    "            else:\n",
    "                optimal_idx = np.asarray(res.results[\"loss\"]).argmin()\n",
    "                optimal_lr = res.results[\"lr\"][optimal_idx]\n",
    "            optuna_logger.info(f\"Using learning rate of {optimal_lr:.3g}\")\n",
    "            # add learning rate artificially\n",
    "            model.hparams.learning_rate = trial.suggest_uniform(\"learning_rate\", optimal_lr, optimal_lr)\n",
    "        else:\n",
    "            model.hparams.learning_rate = trial.suggest_loguniform(\"learning_rate\", *learning_rate_range)\n",
    "\n",
    "        # fit\n",
    "        trainer.fit(model, train_dataloaders=train_dataloaders, val_dataloaders=val_dataloaders)\n",
    "\n",
    "        # report result\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    # setup optuna and run\n",
    "      if study is None:\n",
    "        study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "      study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "      return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Input_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5708e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Daily Date\"] = pd.to_datetime(data[\"Daily Date\"])\n",
    "data\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['series'] = (data['Daily Date'].dt.year - 2001)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time_idx'] = data.groupby('series').cumcount()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420066b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c08dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.filter(['Daily Date','well','series','time_idx'],axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 3\n",
    "max_encoder_length = 12\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"well\",\n",
    "    group_ids=[\"series\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=3,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_unknown_reals=[\"well\"],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"series\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=7)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d19621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "MAE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb974f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"Ranger\"\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=100, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.01,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"Ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd149d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "MAE()(predictions.output, predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import mse_loss\n",
    "# Convert predictions and true values to PyTorch tensors if they are not already\n",
    "y_pred = torch.tensor(predictions.output)\n",
    "y_true = torch.tensor(predictions.y[0])\n",
    "\n",
    "# Calculate MSE\n",
    "mse_value = mse_loss(y_pred, y_true)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_value = torch.sqrt(mse_value)\n",
    "\n",
    "print(\"MSE:\", mse_value.item())\n",
    "print(\"RMSE:\", rmse_value.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b68162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample1",
   "language": "python",
   "name": "sample1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
